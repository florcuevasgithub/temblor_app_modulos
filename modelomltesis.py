# -*- coding: utf-8 -*-
"""ModeloMLTesis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f87jxqeJy4TqCXwjB0pvaRlisby9DIO9
"""

# train_model.py

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import joblib
import warnings

warnings.filterwarnings("ignore", category=UserWarning, module="sklearn.preprocessing._encoders")

print("Iniciando el script de entrenamiento del modelo...")

# --- 1. Cargar el Dataset ---
try:
    df = pd.read_csv('/content/drive/Shareddrives/PROYECTO INTEGRADOR/INFORMACION PARA LEER Y APRENDER/Procesamiento/python-datascience-ia/COLABS TESIS/codigo de enero/DATASET/dataset_temblor.csv - Hoja 1.csv', decimal=',')
    print("Dataset 'dataset_temblor.csv - Hoja 1.csv' cargado exitosamente.")
    print(f"Dimensiones del dataset: {df.shape}")
    print("Primeras 5 filas del dataset:")
    print(df.head())
    print("\nInformación del dataset:")
    df.info()
except FileNotFoundError:
    print("Error: 'dataset_temblor.csv' no encontrado. Asegúrate de que el archivo esté en la misma carpeta.")
    exit()
except Exception as e:
    print(f"Error al cargar el dataset: {e}. Revisa el formato, el separador decimal y los delimitadores.")
    exit()

# --- 2. Definir las Características (X) y la Variable Objetivo (y) ---

feature_cols = [
    'sexo', 'edad', 'mano_medida', 'dedo_medido', # Estas dos son cruciales para la medición
    'Frec_Reposo', 'RMS_Reposo', 'Amp_Reposo',
    'Frec_Postural', 'RMS_Postural', 'Amp_Postural',
    'Frec_Accion', 'RMS_Accion', 'Amp_Accion'
]
target_col = 'diagnostico'

missing_cols = [col for col in feature_cols + [target_col] if col not in df.columns]
if missing_cols:
    print(f"\nError: Faltan las siguientes columnas en tu dataset: {missing_cols}")
    print("Asegúrate de que los nombres de las columnas en tu CSV coincidan exactamente con la lista 'feature_cols' y que incluya 'mano_medida' y 'dedo_medido'.")
    exit()

X = df[feature_cols]
y = df[target_col]

print(f"\nCaracterísticas (X) seleccionadas: {feature_cols}")
print(f"Variable objetivo (y): {target_col}")

# --- 3. Preprocesamiento de Datos ---
# 'dedo_medido' añadido a las categóricas.
categorical_features = ['sexo', 'mano_medida', 'dedo_medido']
numerical_features = [col for col in feature_cols if col not in categorical_features]

print(f"Características categóricas: {categorical_features}")
print(f"Características numéricas: {numerical_features}")

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])

# --- 4. Crear el Pipeline del Modelo ---
model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                                 ('classifier', RandomForestClassifier(random_state=42))])

print("\nPipeline del modelo creado (Preprocesador + RandomForestClassifier).")

# --- 5. Dividir los Datos para Entrenamiento y Prueba ---
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print(f"Datos divididos: Entrenamiento ({len(X_train)} muestras), Prueba ({len(X_test)} muestras).")

# --- 6. Entrenar el Modelo ---
print("\nEntrenando el modelo...")
model_pipeline.fit(X_train, y_train)
print("Modelo entrenado exitosamente.")

# --- 7. Evaluar el Modelo ---
print("\nEvaluando el modelo en el conjunto de prueba...")
y_pred = model_pipeline.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Precisión del modelo en el conjunto de prueba: {accuracy:.4f}")

print("\nInforme de Clasificación:")
print(classification_report(y_test, y_pred))

print("\nMatriz de Confusión:")
print(confusion_matrix(y_test, y_pred))

# --- 8. Exportar el Modelo ---
model_filename = 'tremor_prediction_model.joblib'
joblib.dump(model_pipeline, model_filename)
print(f"\nModelo final guardado como '{model_filename}'")

print("\nScript de entrenamiento completado.")